{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Extracted-Features/blob/master/Experiments_with_HTRC_Feature_Reader_UPDATED_8_1_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YHL_EcFLRx8"
      },
      "source": [
        "# ü™ê Introduction\n",
        "\n",
        "This notebook is an updated version of the HTRC_SF_experiments notebook: https://github.com/gwijthoff/HTRC_SF_experiments/blob/main/htrc_sf_experiments.ipynb \n",
        "\n",
        "In this notebook, you will learn how to analyze over three thousand speculative fiction novels using HathiTrust Research Center (HTRC) Analytics. Rather than working with the complete text of these novels, we will use \"Extracted Features\": a data format devised by HathiTrust in order to enable text analysis on [post-1926 books still under copyright protection](https://en.wikipedia.org/wiki/Public_domain_in_the_United_States).\n",
        "\n",
        "Beginning with a print book that looks like this...\n",
        "\n",
        "<img src=\"img/wells_moon_print.jpg\" alt=\"first page of Wells The First Men in the Moon\" style=\"width: 400px;\"/>\n",
        "\n",
        "...then scanning and OCRing it to grab its text...\n",
        "\n",
        "```\n",
        "THE FIRST MEN IN \n",
        " THE MOON \n",
        " \n",
        " MR. BEDFORD MEETS MR. CAVOR AT LYMPNE \n",
        " \n",
        " As I sit down to write here amidst the \n",
        " shadows of vine-leaves under the blue sky of \n",
        " southern Italy, it comes to me with a certain \n",
        " quality of astonishment that my participation \n",
        " in these amazing adventures of Mr. Cavor \n",
        " was, after all, the outcome of the purest acci- \n",
        " dent. It might have been any one. I fell \n",
        " into these things at a time when I thought \n",
        " myself removed from the slightest possibility \n",
        " of disturbing experiences. I had gone to \n",
        " Lympne because I had imagined it the most \n",
        " uneventful place in the world. \" Here, at any \n",
        " rate,\" said I, \" I shall find peace and a chance \n",
        " to work ! \" \n",
        " ' And this book is the sequel. So utterly at \n",
        "```\n",
        "\n",
        "...HTRC finally transforms that text into Extracted Features: a compressed `.json` file no longer readable by human eyes (\"consumptive\" reading), yet containing \"quantitative abstractions of a book‚Äôs written content\" that we can explore through text analysis (\"non-consumptive\" reading):\n",
        "\n",
        "```json\n",
        "\":1,\"l\":1,\"r\":1,\"o\":1},\"tokenPosCount\":{\"rate\":{\"NN\":1},\"accident\":{\"NN\":1},\"IN\":{\"IN\":1},\"astonishment\":{\"NN\":1},\"down\":{\"RB\":1},\"slightest\":{\"JJS\":1},\"quality\":{\"NN\":1},\"find\":{\"VB\":1},\"disturbing\":{\"JJ\":1},\"AT\":{\"IN\":1},\"vine-leaves\":{\"NNS\":1},\"any\":{\"DT\":2},\"southern\":{\"JJ\":1},\"myself\":{\"PRP\":1},\"have\":{\"VB\":1},\"is\":{\"VBZ\":1},\"MOON\":{\"NN\":1},\"said\":{\"VBD\":1},\"Lympne\":{\"NNP\":1},\"sit\":{\"VBP\":1},\"thought\":{\"VBD\":1},\".\":{\".\":5},\"adventures\":{\"NNS\":1},\"blue\":{\"JJ\":1},\"THE\":{\"DT\":2},\"world\":{\"NN\":1},\"fell\":{\"VBD\":1},\"CAVOR\":{\"NNP\":1},\"all\":{\"DT\":1},\"book\":{\"NN\":1},\"had\":{\"VBD\":2},\"imagined\":{\"VBN\":1},\"it\":{\"PRP\":2},\"!\":{\".\":1},\"A\":{\"DT\":1},\"a\":{\"DT\":3},\"And\":{\"CC\":1},\"utterly\":{\"RB\":1},\"sky\":{\"NN\":1},\"shadows\":{\"NNS\":1},\"outcome\":{\"NN\":1},\"Here\":{\"RB\":1},\"because\":{\"IN\":1},\"Mr.\":{\"NNP\":1},\"purest\":{\"JJS\":1},\"removed\":{\"VBD\":1},\"certain\":{\"JJ\":1},\"comes\":{\"VBZ\":1},\"MEN\":{\"NNP\":1},\"I\":{\"PRP\":7},\"LYMPNE\":{\"NNP\":1},\"work\":{\"VB\":1},\"that\":{\"WDT\":1},\"possibility\":{\"NN\":1},\"to\":{\"TO\":4},\"participation\":{\"NN\":1},\"MEETS\":{\"VBZ\":1},\",\":{\",\":6},\"most\":{\"RBS\":1},\"here\":{\"RB\":1},\"these\":{\"DT\":2},\"was\":{\"VBD\":1},\"at\":{\"IN\":3},\"been\":{\"VBN\":1},\"FIRST\":{\"NNP\":1},\"'\":{\"''\":1},\"my\":{\"PRP$\":1}\n",
        "```\n",
        "\n",
        "Each element you see in the `.json` sample above is a *feature,* a \"quantifiable marker of something measurable, a datum,\" as Peter Organisciak and Boris Capitanu put it in their [*Programming Historian* tutorial](https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) on text mining with HTRC. They continue:\n",
        "\n",
        "> A computer cannot understand the meaning of a sentence implicitly, but it can understand the counts of various words and word forms, or the presence or absence of stylistic markers, from which it can be trained to better understand text. Many text features are non-consumptive in that they don‚Äôt retain enough information to reconstruct the book text.\n",
        "\n",
        "As we'll see, Extracted Features files will allow us not only to count \"tokens\" (words) in each \"volume\" (published book), but also to filter by parts of speech, browse extensive bibliographic metadata, view quantitative information about each printed page in the dataset, use named entity recognition (NER) to identify people, places, or organizations in the text, graph these elements, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQqlJG8uGiln"
      },
      "source": [
        "# ü™ê Overview of HTRC data\n",
        "\n",
        "HTRC provides a number of [specialized, recommended worksets](https://analytics.hathitrust.org/staticrecommendedworksets): genre- or period-specific collections of books (or \"volumes\") that have been digitized by HathiTrust. Here, we'll work with David Mimno and Laure Thompson's **20th Century English-Language Speculative Fiction** workset. It contains 3,236 \"volumes of speculative fiction [from 1900-1999] identified both through matching titles and authors to [Worlds Without End](https://www.worldswithoutend.com/), an extensive fan-built database of speculative fiction, and via computational text similarity analysis techniques.\"\n",
        "\n",
        "As we explore this workset of speculative fiction, it is important to remember the ways that databases and bibliographies of SF are inherently exclusionary. Suzanne Boswell, who uses The Internet Speculative Fiction Database (ISFDB) in a network analysis of women writers in pulp magazines of the 1920s-40s ([article](https://www.liverpooluniversitypress.co.uk/journals/article/61993/) | [data](https://github.com/sfboswell/Gender_Pulps)), argues that standard bibliographies of SF are almost never representative.\n",
        "\n",
        "> The ISFDB chooses what constitutes science fiction, horror, and fantasy by deciding what to archive in its database. For the early twentieth century, this mainly means the pulps. This decision makes it difficult to track the contributions of women to the science fiction genre: if the pulps excluded women, and bibliographic archives only count the pulps as science fiction, where do we find the women? Another example: in the early twentieth century, most science fiction by Black authors was published outside of the pulps (W. E. B. Dubois's ‚ÄúThe Comet‚Äù [1920]; Pauline Hopkins's *Of One Blood* [1902‚Äì1903]; George Schuyler's short stories in *The Pittsburgh Courier* [1936‚Äì1937]). The ISFDB will have the bibliographic information for, say, George Schuyler‚Äîbut it does not have bibliographic information for other Black-dominated magazines, or Black fiction pamphlets, where other Black speculative fiction writers may exist outside of sf archives. Marginalized authors who write outside the pulps enter science fiction archives as exceptions: their community does not come with them. In this way, science fiction archives repeat the exclusionary patterns of the early twentieth century.\n",
        "\n",
        "Building text corpora using databases like WWE and ISFDB can severely limit the diverse range of voices that contributed to 20th-century speculative fiction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY4glmfaGilo"
      },
      "source": [
        "# ü™ê Downloading HTRC data\n",
        "\n",
        "The [HTRC Analytics interface](https://analytics.hathitrust.org) provides a few different, pre-defined ways of accessing, analyzing, and downloading the data in their recommended worksets. I have already pre-downloaded all of the files needed for this tutorial and included them in this GitHub repository. So, if it's speculative fiction you want to work with, feel free to move on to the tutorial's [next ü™ê section](#Working-with-metadata-for-entire-SF-workset) to explore that data. Otherwise, if you would like to use one of HTRC's recommended worksets, create one of your own, or learn the steps I took to get these files, continue reading below the line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ_GPr2jGilp"
      },
      "source": [
        "***********\n",
        "\n",
        "In order to download our volumes, we'll use Feature Reader, a Python library for working with the HTRC Extracted Features dataset.\n",
        "\n",
        "For future reference: another way of downloading and analyzing Extracted Features is by generating an `rsync` file on HathiTrust's website. You can browse a number of recommended, genre- or period-specific worksets by navigating to <https://analytics.hathitrust.org/worksets> and logging in or create an account. Here are the full [instructions for doing so](https://analytics.hathitrust.org/algorithms/Extracted_Features_Download_Helper?wsname=20th+Century+English-Language+Speculative+Fiction%40htrc). That process generates a shell script, `EF_Rsync.sh`. Once downloaded and run, that script will download the Extracted Features files for all volumes in that workset, storing them in a tricky pairtree file/folder format. Using that process, you can also download pre-processed analyses of the texts in the workset, including named entity recognition, token counts, and topic modeling.\n",
        "\n",
        "But in this tutorial, we'll do the downloading right in this notebook using Python. Again, *I've already done so and downloaded all of the Extracted Features data into the [`data/SF_Extracted_Features`](https://github.com/gwijthoff/HTRC_SF_experiments/tree/main/data/SF_Extracted_Features) directory of this repo.*\n",
        "\n",
        "Thompson and Mimno's original list actually included 5,168 identified volumes of SF in HathiTrust, but there many duplicate copies (i.e. the [1998 Anchor Books edition](https://catalog.hathitrust.org/Record/004963699) and the [2006 Everyman edition](https://catalog.hathitrust.org/Record/009819360) of Margaret Atwood's *The Handmaid's Tale.* So, let's begin by reading a .tsv file of all those volumes using [Pandas](https://pandas.pydata.org/), a Python library for working with tabular data. Then let's remove the duplicates."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TH0e7KnuG0or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX-oZdzCGilq"
      },
      "outputs": [],
      "source": [
        "# create dataframe from Thomson and Mimno's full list of 5,168 SF volumes\n",
        "import pandas as pd\n",
        "df = pd.read_csv('thompson-mimno-SF-final-matches.tsv', delimiter='\t')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwYuuXwiGilt"
      },
      "outputs": [],
      "source": [
        "# remove duplicate titles, leaving us with 3,236 unique works\n",
        "df_minus_duplicates = df.drop_duplicates(subset=['Title'])\n",
        "df_minus_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahaya_cFGilu"
      },
      "source": [
        "GitHub limits us to storing 1,000 files per folder, so let's take a random sample of 1,000 of these volumes to download and work with. *If you are running this notebook and downloading locally, you can skip this stem and download all 3,236 volumes!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyqA-8H5Gilv"
      },
      "outputs": [],
      "source": [
        "# create a new dataframe with 1,000 volumes randomly sampled from our list\n",
        "# random_state=1 keeps the same 1,000 works, rather than re-sampling\n",
        "df_sample = df_minus_duplicates.sample(n=1000, random_state=1)\n",
        "df_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYhLrvzCGilw"
      },
      "outputs": [],
      "source": [
        "# create a list of the HathiTrust IDs column\n",
        "sf_ids = df_sample['HTID'].tolist()\n",
        "sf_ids[:10] # view the first 10 results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb0c2vgSGilx"
      },
      "source": [
        "Next we need to install HTRC's Feature Reader, the Python library that will allow us to download and explore the Extracted Features data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDMra7tzGilx"
      },
      "outputs": [],
      "source": [
        "# install the HTRC Feature Reader\n",
        "! pip install htrc-feature-reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj63vzvcGily"
      },
      "source": [
        "We can now download the Extracted Features files. Altogether, these 1,000 volumes will be 164.5 MB and will take a few minutes to download. If you're downloading all 3,236 volumes, it will be 534 MB. (For details on the download process and some other methods for doing so, see [HTRC's documentation on downloading Extracted Features](https://github.com/htrc/htrc-feature-reader/blob/master/examples/ID_to_Rsync_Link.ipynb).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyiUHnzhGilz"
      },
      "outputs": [],
      "source": [
        "# download Extracted Features files to a subdirectory in our data folder\n",
        "#ONLY RUN THIS ONCE!\n",
        "#Takes a couple of minutes to load\n",
        "from htrc_features import utils\n",
        "utils.download_file(htids=sf_ids, outdir = '/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6TD7amdGilz"
      },
      "source": [
        "**********************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLaME5RYGil0"
      },
      "source": [
        "# ü™ê Pulling data on one volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK223fF4Gil0"
      },
      "outputs": [],
      "source": [
        "# Recall the first dataframe we created...\n",
        "df_sample.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKYfTkTdGil1"
      },
      "outputs": [],
      "source": [
        "# let's reset the index to number each row\n",
        "SFlist = df_sample.reset_index().drop(columns='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB5jkP57Gil1"
      },
      "outputs": [],
      "source": [
        "SFlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TxAQITxGil2"
      },
      "source": [
        "We could use a HathiTrust ID to pull Extracted Features information online. For example, taking the dataframe index 996 (the number in the leftmost column) for Stanis≈Çaw Lem's *Solaris,* using the ID in the first column `mdp.39015002969064`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5B8F21pGil2"
      },
      "outputs": [],
      "source": [
        "from htrc_features import Volume\n",
        "vol = Volume(\"mdp.39015002969064\")\n",
        "vol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za7KNTNFGil2"
      },
      "source": [
        "However, we want to work with the local files we just downloaded. So, we need to create a new column in our dataframe that points to the path for the Extracted Features file of each volume. Each file uses a HathiTrust ID, needs to be prefaced by its directory location `data/SF_Extracted_Features/`, and suffixed with its filetype `.json.bz2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNhJfHgiGil3"
      },
      "outputs": [],
      "source": [
        "# create a new column combining directory location and filetype strings\n",
        "# with existing HTID column\n",
        "SFlist['Initial Path'] = SFlist['HTID'] + '.json.bz2'\n",
        "SFlist.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_-K_i6sGil3"
      },
      "source": [
        "One thing to note is that some HathiTrust IDs contain `:` and `/` characters, which imply local directory locations. When downloading Extracted Features files, Feature Reader replaces those characters with `+` and `=`, respectively. For those volumes, we need to replace those characters to reflect the actual location. For example, Ursula K. Leguin's *A Wizard of Earthsea,* will be changed in the `Initial Path` column from\n",
        "\n",
        "`dul1.ark:/13960/t51g5w741.json.bz2`\n",
        "\n",
        "to \n",
        "\n",
        "`dul1.ark+=13960=t51g5w741.json.bz2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Suxcm3MGil4"
      },
      "outputs": [],
      "source": [
        "# to see which files contain those characters\n",
        "# SFlist[SFlist['HTID'].str.contains('/')]\n",
        "\n",
        "# create a new Path column for our final location\n",
        "SFlist['Path'] = SFlist['HTID'].str.replace(':','+').str.replace('/','=')\n",
        "\n",
        "# add directory slugs and file type\n",
        "SFlist['Path'] = SFlist['Path'] + '.json.bz2'\n",
        "\n",
        "# remove Initial Path column with incorrect location\n",
        "SFlist = SFlist.drop(columns=['Initial Path'])\n",
        "\n",
        "# verify that the Le Guin volume path is now correct\n",
        "SFlist[SFlist['Title'].str.contains('Earthsea')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwNbrkQRGil5"
      },
      "source": [
        "Now, we can select the file path for one row in our dataframe, and pull the Extracted Features for that volume. We'll use `iloc` to select a cell at row 996 and column 5 (counting over from the left beginning at 0). Once we have that local file path, we can access the Extracted Features data for that volume using Feature Reader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiEdiSDVGil5"
      },
      "outputs": [],
      "source": [
        "# select line 996 for Solaris and column 5 for the file path\n",
        "lem_path = SFlist.iloc[996,5]\n",
        "# pull the Extracted Features\n",
        "vol = Volume(lem_path)\n",
        "vol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH5T5F95Gil5"
      },
      "source": [
        "And we can interact with the Extracted Features in many ways. For example, we can pull various metadata fields and insert them into a string of text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8LtFtkWGil6"
      },
      "outputs": [],
      "source": [
        "print(f'There are {vol.page_count}pp. in {vol.title}, which was published in {vol.pub_place} and can be identified by OCLC number {vol.oclc}.')\n",
        "print(f'This copy of {vol.title} was originally digitized at {vol.source_institution}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEooQVBKGil6"
      },
      "source": [
        "To view a list of all available metadata fields for this volume (others will have more or less metadata available), run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwj_4UuYGil6"
      },
      "outputs": [],
      "source": [
        "vol.parser.meta.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWqOrRS8Gil7"
      },
      "source": [
        "To view the values for any of those fields, simply enter `vol.FIELD`. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzncU4BpGil7"
      },
      "outputs": [],
      "source": [
        "# date of the latest copyright update for this book\n",
        "vol.last_rights_update_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxLkuXLtGil7"
      },
      "outputs": [],
      "source": [
        "# count the number of tokens per page\n",
        "tokenspp = vol.tokens_per_page()\n",
        "tokenspp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu2IatGUGil8"
      },
      "outputs": [],
      "source": [
        "# graph the count of tokens per page over the course of the book\n",
        "tokenspp.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMW_EhHQGil8"
      },
      "source": [
        "Return details on each token's part of speech (represented by the [Penn Tree Bank](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for English), number of times it appears in the book, and the section it appears in the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBsW_wUtGil8"
      },
      "outputs": [],
      "source": [
        "tl = vol.tokenlist()\n",
        "tl.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL4fH-KVGil9"
      },
      "outputs": [],
      "source": [
        "# create a list of all the unique tokens / words in the book\n",
        "words = vol.tokens()\n",
        "# count those words / measure size of the book's vocabulary\n",
        "print(len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT5wDiMJGil9"
      },
      "outputs": [],
      "source": [
        "# view other section features, counting number of tokens, \n",
        "# lines, and sentences on each page of the book\n",
        "features = vol.section_features()\n",
        "features[100:110]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü™ê Getting Word Frequencies in One and Multiple Texts"
      ],
      "metadata": {
        "id": "ArLnomsD0GzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get all occurrencecs of a word in one text (Solaris, example text from above)\n",
        "count_df = tl.loc[(slice(None), slice(None), \"space\")]\n",
        "count_df.head()"
      ],
      "metadata": {
        "id": "wwHHzs8g0HT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get sum of all occurences of space in Solaris\n",
        "count = count_df['count'].sum()\n",
        "\n",
        "#Print results of word count\n",
        "print(f'The word \"space\" appears {count} times in {vol.title}')"
      ],
      "metadata": {
        "id": "gXXVw_Wo1Dvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get multiple texts to compare word frequency\n",
        "##Could use whole corpus but it takes a very long time to run (also need to fix errors when word is not used)\n",
        "##Could select specific files, here just comparing a random sample\n",
        "df = SFlist.sample(5)\n",
        "df"
      ],
      "metadata": {
        "id": "KfqAmvJ45fHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare new dataframe to add counts--we only need title, author and path\n",
        "df= df.drop(columns=['HTID','WWE Novel ID', 'Hand / Auto?'])\n",
        "df"
      ],
      "metadata": {
        "id": "FMfdmtI252WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get all occurences of a word in the selected texts\n",
        "texts = df\n",
        "space_count = []\n",
        "\n",
        "for item in texts['Path']:\n",
        "    vol = Volume(item)\n",
        "    tl = vol.tokenlist()\n",
        "    count_df = tl.loc[(slice(None), slice(None), \"space\")]\n",
        "    space_count.append(count_df['count'].sum())\n",
        "    "
      ],
      "metadata": {
        "id": "rNYsf5bJ2z_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that counts have been tallied\n",
        "space_count[:5]"
      ],
      "metadata": {
        "id": "4R-aWFZH4YCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Append title and sum of  occurences to dataframe\n",
        "#To make this neater, could drop columns so only title and count are left\n",
        "df['Space Count'] = space_count\n",
        "df"
      ],
      "metadata": {
        "id": "0sYNrN2U12Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bar plot based on frequency \n",
        "from matplotlib.pyplot import *\n",
        "\n",
        "df.plot.bar(x=\"Title\", y=\"Space Count\", rot=70, title=\"Frequency of 'Space' in SF Texts\");\n",
        "\n",
        "plot(block=True)\n"
      ],
      "metadata": {
        "id": "Nale0arn6spT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGgnljyWGil9"
      },
      "source": [
        "# ü™ê Enriching our bibliography\n",
        "\n",
        "Next, we can pull some of that Extracted Features data for each volume back into `SFlist`, the dataframe of all the volumes we're working with. We'll create a new dataframe for all of this bibliographic data, calling it `SFbib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ3uz46YGil-"
      },
      "outputs": [],
      "source": [
        "# clean up the df, dropping columns we won't use\n",
        "SFbib = SFlist.drop(columns=['WWE Novel ID', 'Hand / Auto?'])\n",
        "SFbib.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn7V9oyMGil-"
      },
      "outputs": [],
      "source": [
        "# for some reason it looks like\n",
        "# mdp.39015058731913 is not in the EF dataset\n",
        "# and returns non-zero exit status 23 when trying to dl\n",
        "# so let's remove it. index #868 in the current DF\n",
        "SFbib = SFbib.drop(868)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeEwroeCGil-"
      },
      "source": [
        "Let's loop through each line of our dataframe, pulling the published year of each volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O09uLpOQGil_"
      },
      "outputs": [],
      "source": [
        "# Attempt at fixing above to pull pub years \n",
        "# Append all pub years to list and then put years into new column --but are years aligned with correct titles?\n",
        "\n",
        "test = SFbib\n",
        "years = []\n",
        "\n",
        "for item in test['Path']:\n",
        "    vol = Volume(item)\n",
        "    years.append(vol.year)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['Pub Year'] = years\n",
        "test"
      ],
      "metadata": {
        "id": "VH8CwjSFOt2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2PqgKQvGil_"
      },
      "outputs": [],
      "source": [
        "# Graph the distribution of publication years for volumes in the list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = test['Pub Year'].value_counts()"
      ],
      "metadata": {
        "id": "rOd2KrKQjK7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('retina', quality=100)"
      ],
      "metadata": {
        "id": "4m12KvSUKJgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create bar plot based on publication years \n",
        "from matplotlib.pyplot import *\n",
        "# Importing the matplotlib library\n",
        "import matplotlib.pyplot as plt\n",
        "# Declaring the figure or the plot (y, x) or (width, height)\n",
        "plt.figure(figsize = (12,7))\n",
        "\n",
        "x=\"Year\"\n",
        "y=\"Year Count\"\n",
        "\n",
        "default_x_ticks = range(len(x))\n",
        "plt.plot.bar(default_x_ticks, y, title=\"Titles Published Per Year\")\n",
        "\n",
        "plot(block=True)"
      ],
      "metadata": {
        "id": "8eH3rcKCjBoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9vtF9jVpbPO"
      },
      "source": [
        "# ü™ê Filtering our new bibliographic dataframe\n",
        "\n",
        "We can now use this DataFrame containing bibliographic and file path info to query Extracted Features data on each book.\n",
        "\n",
        "Let's filter the author column to all volumes containing the string \"Delany,\" to find works included in this list written by Samuel R. Delany."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9yGo1eyazmg"
      },
      "outputs": [],
      "source": [
        "SFbib[SFbib['Author'].str.contains('Delany')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOzmBclzk7A"
      },
      "source": [
        "# ü™ê What's next\n",
        "\n",
        "Check out these tutorials for more examples of how to work with Extracted Features data and HTRC's Feature Reader.\n",
        "\n",
        "- [README.ipynb](https://github.com/htrc/htrc-feature-reader/blob/master/README.ipynb) from the HTRC github documentation\n",
        "- [Text Mining in Python through the HTRC Feature Reader](https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) from the *Programming Historian*\n",
        "- [Analyzing Documents with TF-IDF](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) at the *Programming Historian* also uses HTRC Extracted Features\n",
        "- visualize the rise and fall of topic models across a book with [htrc-book-models](https://github.com/organisciak/htrc-book-models)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Experiments with HTRC Feature Reader UPDATED 8-1-2022.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}