{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Sectioning and Disaggregation in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsG8HSErXqZR20E1VCVLwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Extracted-Features/blob/master/Text_Sectioning_and_Disaggregation_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "Use this code to clean, section, and disaggregate texts and corpora. \n",
        "\n",
        "**Why Perform Text Sectioning?** \n",
        "\n",
        "Dividing texts into sections (for example, chapters or chunks of N length) is valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
        "\n",
        "**Why Disaggregate Texts?** \n",
        "\n",
        "The process of disaggregating the words in texts (in this case, by alphabetizing them) also creates data sets that can be shared freely where original texts cannot be due to copyright restrictions. \n",
        "\n",
        "*Input/Output Specifications:* \n",
        "\n",
        "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
      ],
      "metadata": {
        "id": "9sOwRC3vIdVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload and Add Text Files To Pandas DataFrame\n",
        "In this section, text files are mounted to Google Drive from the local machine and then loaded into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
      ],
      "metadata": {
        "id": "lFlH8OZYDYS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDahFTGchibT"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet all files to upload\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "c87z_j2mJECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "books = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "books.head()"
      ],
      "metadata": {
        "id": "KhEVwGYLNkwZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "books = books.reset_index()\n",
        "books.columns = [\"Title\", \"Text\"]\n",
        "books"
      ],
      "metadata": {
        "id": "mUxVfRoqRebA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "books['Text'] = books['Text'].apply(lambda x: x.decode('utf-8'))"
      ],
      "metadata": {
        "id": "6OWjVaqa84av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change data type to string (to support further cleaning below)\n",
        "books = books.astype(str)"
      ],
      "metadata": {
        "id": "Rh1LAZH7VzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set dataframes as Colab data tables\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "tYYKh7vqRKLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean Texts and Set Parameters for Sectioning\n",
        "Several basic cleaning processes are implemented: removing unwanted characters from titles, removing newline characters from texts, and removing punctuation. Parameters are also set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
      ],
      "metadata": {
        "id": "-74TxYNkiGbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titles\n",
        "books['Title'] = books['Title'].str.replace(r'.txt', ' ', regex=True) \n",
        "books.head()"
      ],
      "metadata": {
        "id": "Dlt6Q1VJv-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newline characters\n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "7j87ozc4RLrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\-\\.\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\-\\s]+', ' ', regex = True)\n",
        "books.head()"
      ],
      "metadata": {
        "id": "h_rDtxQq8Utc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove paratext (before and after START OF BOOK and END OF BOOK tags)\n",
        "#If texts you are working with do not have these tags, ignore this cell\n",
        "\n",
        "#Split book on start of book tag, keep text only after start of book tag\n",
        "start = books[\"Text\"].str.split(\"START OF BOOK\", expand = True)\n",
        "books['Text'] = start[1]\n",
        "\n",
        "#Split book on end of book tag, keep text only before of book tag\n",
        "end = books[\"Text\"].str.split(\"END OF BOOK\", expand = True)\n",
        "books['Text'] = end[0]\n",
        "books"
      ],
      "metadata": {
        "id": "Kys5CJNyswrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that text is cleaned and sectioned\n",
        "books.iloc[0]['Text']"
      ],
      "metadata": {
        "id": "ob2j0r6YZsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define new dataframe\n",
        "books_cleaned = books"
      ],
      "metadata": {
        "id": "Wlyjxbp_hTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section Texts By Chapter Headings\n",
        "When working with texts with clearly delineated chapters, using chapter headings is a relatively easy way to section texts into segments of (relatively) the same size. After checking the chapter counts for each text to confirm whether sectioning by chapter is a useful procedure, this code iterates through the texts and splits them each time it encounters a new \"chapter\" heading. From here, the text from each chapter is appended to a new dataframe and denoted by book and chapter number. "
      ],
      "metadata": {
        "id": "cj4jSFppS97y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optinal-disable colab dataframe to view # data more easily\n",
        "from google.colab import data_table\n",
        "data_table.disable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "t8Cc5NkSVf2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of chapters in each text\n",
        "chapter_counts = books_cleaned['Text'].str.count('CHAPTER')\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Chapters\"] = chapter_counts\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "Smrrk7fOhYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new chapter starts \n",
        "new = books_cleaned[\"Text\"].str.split(\"CHAPTER\", expand = True).set_index(books_cleaned['Title'])\n",
        "new"
      ],
      "metadata": {
        "id": "8bH_ZGrGjRHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "chapters_df = new.stack().reset_index()\n",
        "chapters_df.columns = [\"Book\", \"Chapter\", \"Text\"]\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "pRvTfDVZq7O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chapter labels into one column\n",
        "chapters_df['Book + Chapter'] = chapters_df['Book'].astype(str) + '_Chapter_' + chapters_df['Chapter'].astype(str)\n",
        "\n",
        "#Remove individual book and chapter columns\n",
        "chapters_df.drop(columns=['Book', 'Chapter'])\n",
        "\n",
        "#Lowercase all words\n",
        "chapters_df['Text'] = chapters_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chapter is first column \n",
        "column_names = \"Book + Chapter\", \"Text\"\n",
        "chapters_df = chapters_df.reindex(columns=column_names)\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "AtjYB_112gee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section Chapters by Chunks of N Length\n",
        "Though chapter headings are useful for splitting texts into semi-equal segments, disparities in chapter length may occur, especially in large corpora. To further segment texts, the text of each text can be divided into chunks of n length. "
      ],
      "metadata": {
        "id": "d-7KVlpthXaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new df to work with chunks\n",
        "new_chapters_df = chapters_df\n",
        "\n",
        "#Get number of words in each chapter (helps to determine chunk length)\n",
        "ch_words = new_chapters_df[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append word counts to dataframe\n",
        "new_chapters_df[\"Word Count\"] = ch_words\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "Y1JiAjSDhW94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "new_chapters_df['Tokens'] = new_chapters_df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "-TB1hNp5iDrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_ch = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = new_chapters_df['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Add to new list\n",
        "  chunked_ch.append(chunks)\n"
      ],
      "metadata": {
        "id": "-8rQnPbRiN2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = new_chapters_df['Book + Chapter']\n",
        "values = chunked_ch\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ],
      "metadata": {
        "id": "Z4xilhl1ibkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_ch_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_ch_df.head()"
      ],
      "metadata": {
        "id": "P8TGYS50il1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_ch_df = chunked_ch_df.stack().reset_index()\n",
        "chunked_ch_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "vDkkG11gipwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_ch_df['Book + Chunk'] = chunked_ch_df['Title'].astype(str) + ' Chunk ' + chunked_ch_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_ch_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "TreebankWordDetokenizer().detokenize\n",
        "\n",
        "chunked_ch_df['Text'] = chunked_ch_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_ch_df['Text'] \n",
        "\n",
        "#Lowercase all words\n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_ch_df = chunked_ch_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "qOwtYw0hi6_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section Texts By Chunks of N Length\n",
        "When working with texts WITHOUT discernable chapter headings--or, even if chapter headings are present but too infrequent to split texts into meaningful segments--texts can instead be sectioned by chunks of \"N\" length, where N is a variable that can be custom-set below. After checking the word counts for each text to determine what size chunks would be appropriate, this code iterates through the texts and splits them each time it counts \"N\" number of words. From here, the text from each chunk is appended to a new dataframe and denoted by book and chunk number."
      ],
      "metadata": {
        "id": "wS7KWmxq3HQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get number of words in each book (helps to determine chunk length)\n",
        "words = books_cleaned[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Word Count\"] = words\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "yWE_NqN-CO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "books_cleaned['Tokens'] = books_cleaned.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "XxuTbkEZNKhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_sentences = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = books_cleaned['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Check that text is being chunked correctly\n",
        "  print(chunks[0])\n",
        "  #Add to new list\n",
        "  chunked_sentences.append(chunks)\n"
      ],
      "metadata": {
        "id": "8GLee_W4ZC4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = books_cleaned['Title']\n",
        "values = chunked_sentences\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ],
      "metadata": {
        "id": "xcgktYQzb34m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_df.head()"
      ],
      "metadata": {
        "id": "40ruZQhgc3AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_df = chunked_df.stack().reset_index()\n",
        "chunked_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "CdjZTi9adr2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_df['Book + Chunk'] = chunked_df['Title'].astype(str) + ' Chunk ' + chunked_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "TreebankWordDetokenizer().detokenize\n",
        "\n",
        "chunked_df['Text'] = chunked_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_df['Text'] \n",
        "\n",
        "#Lowercase all words\n",
        "chunked_df['Text'] = chunked_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_df = chunked_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "NSnpX8s8enM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Disaggregate Texts and Download CSV Output\n",
        "Working with texts split by chapter or chunk (or both), the final step of this process is to disaggregate the data. Disaggregation, or the breakdown of data into smaller (disordered) parts, is accomplished through the alphabetization of the words in each chapter/chunk. \n",
        "\n",
        "The resulting \"bag of words\" data can then be downloaded as csvs and used for further analysis, such as through the Topic Modeling pipeline in the Extracted Features repository: https://github.com/SF-Nexus/Extracted-Features/blob/main/Topic%20Modeling%20with%20SciFi%20Corpus.ipynb "
      ],
      "metadata": {
        "id": "Q1A_b2-H0iah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texts Sectioned by Chapter"
      ],
      "metadata": {
        "id": "bfxTcqvYjMmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHAPTER\n",
        "#Alphabetize words in each chapter string\n",
        "chapters_df['Text'] = chapters_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "KnuyMCaTuIcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chapters to csv\n",
        "from google.colab import files\n",
        "\n",
        "chapters_df.to_csv('chapters_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapters_bow_output.csv')"
      ],
      "metadata": {
        "id": "fqWPBvnK44ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texts Sectioned by Chapter + Chunk"
      ],
      "metadata": {
        "id": "8LYQ1-NsjRat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "1pwgXKNBjQnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_ch_df.to_csv('chapter_chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapter_chunks_bow_output.csv')"
      ],
      "metadata": {
        "id": "cjAHRphsjUqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texts Sectioned by Chunk"
      ],
      "metadata": {
        "id": "gx5K0DrijVm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "VfFCzNAO0kNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_df.to_csv('chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chunks_bow_output.csv')"
      ],
      "metadata": {
        "id": "7oZtM8aS2DBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}