{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "From Texts to Topic Models-A Complete Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "40KVswYbunE3",
        "f4qStMOwuvYK",
        "MI34VMFq801e"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Extracted-Features/blob/master/From_Texts_to_Topic_Models_A_Complete_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXOw5gPaRLuQ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Use this code to convert any corpus of plain text files to disaggregated data and then use that data to create LDA topic models. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Packages"
      ],
      "metadata": {
        "id": "jfm_YfxIRWjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "GNIhKdGeRWH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "iTVMYYbCRdhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "metadata": {
        "id": "MvASJDYpRfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel"
      ],
      "metadata": {
        "id": "FZkNZc4lRhTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "bG2pqtVN_DFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "EjAj_KoORnfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Text Sectioning and Disaggregation\n",
        "\n",
        "Use this code to clean, section, and disaggregate texts and corpora. \n",
        "\n",
        "**Why Perform Text Sectioning?** \n",
        "\n",
        "Dividing texts into sections (for example, chapters or chunks of N length) is valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
        "\n",
        "**Why Disaggregate Texts?** \n",
        "\n",
        "The process of disaggregating the words in texts (in this case, by alphabetizing them) also creates data sets that can be shared freely where original texts cannot be due to copyright restrictions. \n",
        "\n",
        "*Input/Output Specifications:* \n",
        "\n",
        "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
      ],
      "metadata": {
        "id": "9sOwRC3vIdVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upload and Add Text Files To Pandas DataFrame\n",
        "In this section, text files are mounted to Google Drive from the local machine and then loaded into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
      ],
      "metadata": {
        "id": "lFlH8OZYDYS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDahFTGchibT"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet all files to upload\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "c87z_j2mJECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "books = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "books"
      ],
      "metadata": {
        "id": "KhEVwGYLNkwZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "books = books.reset_index()\n",
        "books.columns = [\"Title\", \"Text\"]\n",
        "books"
      ],
      "metadata": {
        "id": "mUxVfRoqRebA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "books['Text'] = books['Text'].apply(lambda x: x.decode('utf-8'))"
      ],
      "metadata": {
        "id": "6OWjVaqa84av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change data type to string (to support further cleaning below)\n",
        "books = books.astype(str)"
      ],
      "metadata": {
        "id": "Rh1LAZH7VzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clean Texts and Set Parameters for Sectioning\n",
        "Several basic cleaning processes are implemented: removing unwanted characters from titles, removing newline characters from texts, and removing punctuation. Parameters are also set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
      ],
      "metadata": {
        "id": "-74TxYNkiGbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titles\n",
        "books['Title'] = books['Title'].str.replace(r'.txt', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "Dlt6Q1VJv-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newline characters\n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "_S2txHwRX5vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\s]+', '', regex = True)"
      ],
      "metadata": {
        "id": "h_rDtxQq8Utc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove paratext (before and after START OF BOOK and END OF BOOK tags)\n",
        "#If texts you are working with do not have these tags, ignore this cell\n",
        "\n",
        "#Split book on start of book tag, keep text only after start of book tag\n",
        "start = books[\"Text\"].str.split(\"START OF BOOK\", expand = True)\n",
        "books['Text'] = start[1]\n",
        "\n",
        "#Split book on end of book tag, keep text only before of book tag\n",
        "end = books[\"Text\"].str.split(\"END OF BOOK\", expand = True)\n",
        "books['Text'] = end[0]\n",
        "books"
      ],
      "metadata": {
        "id": "Kys5CJNyswrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that text is cleaned and sectioned\n",
        "books.iloc[0]['Text']"
      ],
      "metadata": {
        "id": "ob2j0r6YZsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define new dataframe\n",
        "books_cleaned = books"
      ],
      "metadata": {
        "id": "Wlyjxbp_hTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section Texts By Chapter Headings\n",
        "When working with texts with clearly delineated chapters, using chapter headings is a relatively easy way to section texts into segments of (relatively) the same size. After checking the chapter counts for each text to confirm whether sectioning by chapter is a useful procedure, this code iterates through the texts and splits them each time it encounters a new \"chapter\" heading. From here, the text from each chapter is appended to a new dataframe and denoted by book and chapter number. "
      ],
      "metadata": {
        "id": "cj4jSFppS97y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of chapters in each text\n",
        "chapter_counts = books_cleaned['Text'].str.count('CHAPTER')\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Chapters\"] = chapter_counts\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "Smrrk7fOhYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new chapter starts \n",
        "new = books_cleaned[\"Text\"].str.split(\"CHAPTER\", expand = True).set_index(books_cleaned['Title'])\n",
        "new"
      ],
      "metadata": {
        "id": "8bH_ZGrGjRHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "chapters_df = new.stack().reset_index()\n",
        "chapters_df.columns = [\"Book\", \"Chapter\", \"Text\"]\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "pRvTfDVZq7O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chapter labels into one column\n",
        "chapters_df['Book + Chapter'] = chapters_df['Book'].astype(str) + '_Chapter_' + chapters_df['Chapter'].astype(str)\n",
        "\n",
        "#Remove individual book and chapter columns\n",
        "chapters_df.drop(columns=['Book', 'Chapter'])\n",
        "\n",
        "#Reindex so book + chapter is first column \n",
        "column_names = \"Book + Chapter\", \"Text\"\n",
        "chapters_df = chapters_df.reindex(columns=column_names)\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "AtjYB_112gee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section Texts By Chunk of N Length\n",
        "When working with texts WITHOUT discernable chapter headings--or, even if chapter headings are present but too infrequent to split texts into meaningful segments--texts can instead be sectioned by chunks of \"N\" length, where N is a variable that can be custom-set below. After checking the word counts for each text to determine what size chunks would be appropriate, this code iterates through the texts and splits them each time it counts \"N\" number of words. From here, the text from each chunk is appended to a new dataframe and denoted by book and chunk number."
      ],
      "metadata": {
        "id": "wS7KWmxq3HQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get number of words in each book (helps to determine chunk length)\n",
        "words = books_cleaned[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append word counts to dataframe\n",
        "books_cleaned[\"Word Count\"] = words\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "yWE_NqN-CO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "books_cleaned['Tokens'] = books_cleaned.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "XxuTbkEZNKhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_sentences = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = books_cleaned['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Check that text is being chunked correctly\n",
        "  print(chunks[0])\n",
        "  #Convert from tokens to sentences to add to new df\n",
        "  for chunk in chunks:\n",
        "    sentences = ' '.join(chunk)\n",
        "    chunked_sentences.append(sentences)"
      ],
      "metadata": {
        "id": "zYZTN17_uAiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe to store chunked sentences for disaggregation\n",
        "#NEEDS WORK--\"Title\" does not reflect which chunks belong to which books\n",
        "df = pd.DataFrame(chunked_sentences).T\n",
        "chunked_df = df.stack().reset_index()\n",
        "chunked_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "Z3lan4X_9aS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_df['Book + Chunk'] = chunked_df['Title'].astype(str) + ' Chunk ' + chunked_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_df = chunked_df.reindex(columns=column_names)\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "PJiT1J4F2nMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Disaggregate Texts and Download CSV Output\n",
        "Working with texts split by chapter or chunk, the final step of this process is to disaggregate the data. Disaggregation, or the breakdown of data into smaller (disordered) parts, is accomplished through the alphabetization of the words in each chapter/chunk. \n",
        "\n",
        "The resulting \"bag of words\" data can then be downloaded as csvs and used for further analysis, such as through the Topic Modeling pipeline in the Extracted Features repository: https://github.com/SF-Nexus/Extracted-Features/blob/main/Topic%20Modeling%20with%20SciFi%20Corpus.ipynb "
      ],
      "metadata": {
        "id": "Q1A_b2-H0iah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHAPTER\n",
        "#Alphabetize words in each chapter string\n",
        "chapters_df['Text'] = chapters_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "KnuyMCaTuIcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chapters to csv\n",
        "from google.colab import files\n",
        "\n",
        "chapters_df.to_csv('sci-fi-chapters_bag_of_words_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('sci-fi-chapters_bag_of_words_output.csv')"
      ],
      "metadata": {
        "id": "fqWPBvnK44ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "VfFCzNAO0kNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_df.to_csv('sci-fi_word_chunks_bag_of_words_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('sci-fi_word_chunks_bag_of_words_output.csv')"
      ],
      "metadata": {
        "id": "7oZtM8aS2DBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Creating Topic Models\n",
        "What are the most common topics in a corpus of science fiction texts? Which texts tend to address the same topics, and why? These are just a couple of the many questions which this code addresses. Below, Gensim is used to create LDA topic models of a pre-loaded dataframe of texts. \n",
        "\n",
        "This code is adapted from this [Intro to Topic Modeling with Gensim and pyLDAvis](https://github.com/hawc2/text-analysis-with-python/blob/master/Topic_Modeling.ipynb) and works well with input from from the Text Sectioning and Disaggregation code from [this repository](https://github.com/SF-Nexus/Extracted-Features/blob/main/Text_Sectioning_and_Disaggregation_in_Python.ipynb)\n",
        "\n",
        "Topic modeling resources: \n",
        "https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
        "\n",
        "https://github.com/polsci/colab-gensim-mallet/blob/master/topic-modeling-with-colab-gensim-mallet.ipynb\n",
        "\n",
        "http://www.cs.columbia.edu/~blei/papers/Blei2011.pdf\n",
        "\n",
        "https://github.com/laurejt/authorless-tms \n",
        "\n",
        "https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf\n",
        "\n",
        "Data display: https://colab.research.google.com/notebooks/data_table.ipynb#scrollTo=jcQEX_3vHOUz "
      ],
      "metadata": {
        "id": "0oS9cHjheHr1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxEMrysjRHwB"
      },
      "source": [
        "##Convert Disaggregated Data into New DF and Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUM-atjwy0mm"
      },
      "source": [
        "#Convert csv to dataframe\n",
        "#Copy name of csv created above with chunked or chapterized data \n",
        "df = pd.read_csv(io.StringIO(uploaded['sci-fi_word_chunks_bag_of_words_output.csv.csv'].decode('utf-8')))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop any empty cells\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "wpVCBFWB9LrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Book + Chapter']"
      ],
      "metadata": {
        "id": "-9M-ikZoGXjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo-GqUxGy_dY"
      },
      "source": [
        "#Add values in Text column to new list (for further cleaning)\n",
        "data = df.Text.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EocrECQchTEF"
      },
      "source": [
        "#View dataframe as Colab data table\n",
        "%load_ext google.colab.data_table \n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7B4L5MRRFJH"
      },
      "source": [
        "##Clean Texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRgCvQRfOOE"
      },
      "source": [
        "#Define list of stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Add further stopwords by simply \"appending\" desired words to dictionary\n",
        "#stop_words.append('movie')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQXKsWbxrsUg"
      },
      "source": [
        "#Remove punctuation\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPfCl6nHHjT"
      },
      "source": [
        "#Define function to perform simple preprocessing on text\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9saf7oBhoRdy"
      },
      "source": [
        "#Run processing function on texts\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOtbz8fXfgCt"
      },
      "source": [
        "#Define bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4qFCAGQfgvA"
      },
      "source": [
        "#Define stopword removal\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc))\n",
        "if word not in stop_words] for doc in texts]\n",
        "\n",
        "#Define function to make bigrams\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "#def make_trigrams(texts):\n",
        "#   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "#Define function to lemmatize texts\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out = []\n",
        "   for sent in texts:\n",
        "     doc = nlp(\" \".join(sent))\n",
        "     texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRMeqTfHkey6"
      },
      "source": [
        "#Run functions to remove stopwords, make bigrams, and lemmatize text\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
        "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
        "])\n",
        "print(data_lemmatized[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JXqo5CI47kE"
      },
      "source": [
        "##Building Dictionary and Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf46pDnu4CNX"
      },
      "source": [
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EExpaCS7SInJ"
      },
      "source": [
        "##Create Topic Model - Topics 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqbXMdeAHIq5"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20,\n",
        "                                           random_state=100,\n",
        "                                           update_every=2,\n",
        "                                           chunksize=100,\n",
        "                                           passes=20,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##View Keywords of Each Topic in the Model\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "hEWGm3Zq93ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate Complexity and Coherence Score\n",
        "#These measures help determine how good the model is: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel\n",
        "\n",
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "YZ8OwXog-UOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhTNneQHJ1C"
      },
      "source": [
        "#Create Visualization (Save HTML)\n",
        "#The easiest way to create the visualization is to reveal it in the Google Colab notebook and save it as an html file that you can view on your browser. \n",
        "\n",
        "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "\n",
        "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk7mOcrnktEp"
      },
      "source": [
        "pyLDAvis.save_html(vis, '/content/LDAviz.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z83wjLBgsedR"
      },
      "source": [
        "pyLDAvis.display(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find the Optimal Number of Topics"
      ],
      "metadata": {
        "id": "0Vpn-8kM_bKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "dIOUcJpZ_dMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can take a long time to run.\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
      ],
      "metadata": {
        "id": "EM-DAB_1_gd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show graph\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gM2Kc3qd_nB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH4WlWezsHNy"
      },
      "source": [
        "## Topic Modeling Model - 15 Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSu3f89PsGtI"
      },
      "source": [
        "lda_model15 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=15,\n",
        "                                           random_state=100,\n",
        "                                           update_every=2,\n",
        "                                           chunksize=100,\n",
        "                                           passes=20,\n",
        "                                           iterations=200,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lst_Wcwsr-J"
      },
      "source": [
        "# Create Visualization (Save HTML)\n",
        "\n",
        "#The easiest way to create the visualization is to reveal it in the Google Colab notebook and save it as an html file that you can view on your browser. \n",
        "\n",
        "vis15 = gensimvis.prepare(lda_model15, corpus, id2word)\n",
        "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJBuBBcGsr-Q"
      },
      "source": [
        "pyLDAvis.save_html(vis60, '/content/LDAviz15.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LnUVVOJsr-X"
      },
      "source": [
        "pyLDAvis.display(vis15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare: LDA Mallet Model"
      ],
      "metadata": {
        "id": "kvECQwSyBPWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet' "
      ],
      "metadata": {
        "id": "ixMnptbWBjxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get mallet path and run new topic model\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
      ],
      "metadata": {
        "id": "6XpL3g_xCZ8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Topics\n",
        "pprint(ldamallet.show_topics(formatted=False))"
      ],
      "metadata": {
        "id": "jO0i0Nu2Dnc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ],
      "metadata": {
        "id": "yMMl3lvGDwkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create visualization\n",
        "converted_ = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
        "vis_mallet = gensimvis.prepare(converted_, corpus, id2word)"
      ],
      "metadata": {
        "id": "hAjdKO5yD1_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display visualization \n",
        "pyLDAvis.display(vis_mallet)"
      ],
      "metadata": {
        "id": "ltwrP3J2ES6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get ideal number of topics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "G23mc9VBEkF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
      ],
      "metadata": {
        "id": "bKvqxcrhFJ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show graph\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C5wY4LjREu4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Look at Topics Per Document"
      ],
      "metadata": {
        "id": "SM2k1HSvE1v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head()"
      ],
      "metadata": {
        "id": "qDxMmNDeFQPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI34VMFq801e"
      },
      "source": [
        "## Serve Visualization in Browser\n",
        "\n",
        "You can also serve the visualization locally in the browser using the below chunk of code. Beware that caching in your browser and other issues, such as ad-blockers, may require some debugging to get this working on your machine. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn8XNwM58zrM"
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.show(vis)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}