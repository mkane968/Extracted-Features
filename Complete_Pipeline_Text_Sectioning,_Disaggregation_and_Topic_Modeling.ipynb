{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complete Pipeline-Text Sectioning, Disaggregation and Topic Modeling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBwv+VYt0GahjTzZC9UrHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Extracted-Features/blob/master/Complete_Pipeline_Text_Sectioning%2C_Disaggregation_and_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Use this code to convert any corpus of plain text files to disaggregated data and then use that data to create LDA topic models. "
      ],
      "metadata": {
        "id": "4sskjJL3szDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part I: Sectioning Texts by Chapter, Chunk, and Chapter + Chunk\n",
        "**Why Perform Text Sectioning?** \n",
        "\n",
        "Dividing texts into sections (for example, chapters or chunks of N length) is valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
        "\n",
        "*Input/Output Specifications:* \n",
        "\n",
        "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
      ],
      "metadata": {
        "id": "jq_NMb-Js1Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Upload and Add Text Files To Pandas DataFrame\n",
        "In this section, text files are mounted to Google Drive from the local machine and then loaded into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
      ],
      "metadata": {
        "id": "lFlH8OZYDYS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDahFTGchibT"
      },
      "outputs": [],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet all files to upload\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "c87z_j2mJECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "books = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "books.head()"
      ],
      "metadata": {
        "id": "KhEVwGYLNkwZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "books = books.reset_index()\n",
        "books.columns = [\"Title\", \"Text\"]\n",
        "books"
      ],
      "metadata": {
        "id": "mUxVfRoqRebA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "books['Text'] = books['Text'].apply(lambda x: x.decode('utf-8'))"
      ],
      "metadata": {
        "id": "6OWjVaqa84av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change data type to string (to support further cleaning below)\n",
        "books = books.astype(str)"
      ],
      "metadata": {
        "id": "Rh1LAZH7VzRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set dataframes as Colab data tables\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "tYYKh7vqRKLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clean Texts and Set Parameters for Sectioning\n",
        "Several basic cleaning processes are implemented: removing unwanted characters from titles, removing newline characters from texts, and removing punctuation. Parameters are also set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
      ],
      "metadata": {
        "id": "-74TxYNkiGbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titles\n",
        "books['Title'] = books['Title'].str.replace(r'.txt', ' ', regex=True) \n",
        "books.head()"
      ],
      "metadata": {
        "id": "Dlt6Q1VJv-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove newline characters\n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "books['Text'] = books['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "books"
      ],
      "metadata": {
        "id": "7j87ozc4RLrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\-\\.\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "books['Text'] = books['Text'].str.replace(r'[^\\w\\-\\s]+', ' ', regex = True)\n",
        "books.head()"
      ],
      "metadata": {
        "id": "h_rDtxQq8Utc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove paratext (before and after START OF BOOK and END OF BOOK tags)\n",
        "#If texts you are working with do not have these tags, ignore this cell\n",
        "\n",
        "#Split book on start of book tag, keep text only after start of book tag\n",
        "start = books[\"Text\"].str.split(\"START OF BOOK\", expand = True)\n",
        "books['Text'] = start[1]\n",
        "\n",
        "#Split book on end of book tag, keep text only before of book tag\n",
        "end = books[\"Text\"].str.split(\"END OF BOOK\", expand = True)\n",
        "books['Text'] = end[0]\n",
        "books"
      ],
      "metadata": {
        "id": "Kys5CJNyswrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check that text is cleaned and sectioned\n",
        "books.iloc[0]['Text']"
      ],
      "metadata": {
        "id": "ob2j0r6YZsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define new dataframe\n",
        "books_cleaned = books"
      ],
      "metadata": {
        "id": "Wlyjxbp_hTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Section Texts By Chapter Headings\n",
        "When working with texts with clearly delineated chapters, using chapter headings is a relatively easy way to section texts into segments of (relatively) the same size. After checking the chapter counts for each text to confirm whether sectioning by chapter is a useful procedure, this code iterates through the texts and splits them each time it encounters a new \"chapter\" heading. From here, the text from each chapter is appended to a new dataframe and denoted by book and chapter number. "
      ],
      "metadata": {
        "id": "cj4jSFppS97y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optinal-disable colab dataframe to view # data more easily\n",
        "from google.colab import data_table\n",
        "data_table.disable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "t8Cc5NkSVf2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count number of chapters in each text\n",
        "chapter_counts = books_cleaned['Text'].str.count('CHAPTER')\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Chapters\"] = chapter_counts\n",
        "books_cleaned"
      ],
      "metadata": {
        "id": "Smrrk7fOhYwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make new cell each time new chapter starts \n",
        "new = books_cleaned[\"Text\"].str.split(\"CHAPTER\", expand = True).set_index(books_cleaned['Title'])\n",
        "new"
      ],
      "metadata": {
        "id": "8bH_ZGrGjRHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "chapters_df = new.stack().reset_index()\n",
        "chapters_df.columns = [\"Book\", \"Chapter\", \"Text\"]\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "pRvTfDVZq7O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chapter labels into one column\n",
        "chapters_df['Book + Chapter'] = chapters_df['Book'].astype(str) + '_Chapter_' + chapters_df['Chapter'].astype(str)\n",
        "\n",
        "#Remove individual book and chapter columns\n",
        "chapters_df.drop(columns=['Book', 'Chapter'])\n",
        "\n",
        "#Lowercase all words\n",
        "chapters_df['Text'] = chapters_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chapter is first column \n",
        "column_names = \"Book + Chapter\", \"Text\"\n",
        "chapters_df = chapters_df.reindex(columns=column_names)\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "AtjYB_112gee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section Chapters by Chunks of N Length\n",
        "Though chapter headings are useful for splitting texts into semi-equal segments, disparities in chapter length may occur, especially in large corpora. To further segment texts, the text of each text can be divided into chunks of n length. "
      ],
      "metadata": {
        "id": "d-7KVlpthXaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new df to work with chunks\n",
        "new_chapters_df = chapters_df\n",
        "\n",
        "#Get number of words in each chapter (helps to determine chunk length)\n",
        "ch_words = new_chapters_df[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append word counts to dataframe\n",
        "new_chapters_df[\"Word Count\"] = ch_words\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "Y1JiAjSDhW94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "new_chapters_df['Tokens'] = new_chapters_df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "-TB1hNp5iDrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_ch = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = new_chapters_df['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Add to new list\n",
        "  chunked_ch.append(chunks)\n"
      ],
      "metadata": {
        "id": "-8rQnPbRiN2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = new_chapters_df['Book + Chapter']\n",
        "values = chunked_ch\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ],
      "metadata": {
        "id": "Z4xilhl1ibkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_ch_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_ch_df.head()"
      ],
      "metadata": {
        "id": "P8TGYS50il1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_ch_df = chunked_ch_df.stack().reset_index()\n",
        "chunked_ch_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "vDkkG11gipwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_ch_df['Book + Chunk'] = chunked_ch_df['Title'].astype(str) + ' Chunk ' + chunked_ch_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_ch_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "TreebankWordDetokenizer().detokenize\n",
        "\n",
        "chunked_ch_df['Text'] = chunked_ch_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_ch_df['Text'] \n",
        "\n",
        "#Lowercase all words\n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_ch_df = chunked_ch_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "qOwtYw0hi6_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section Chapters by Chunks of N Length\n",
        "Though chapter headings are useful for splitting texts into semi-equal segments, disparities in chapter length may occur, especially in large corpora. To further segment texts, the text of each text can be divided into chunks of n length. "
      ],
      "metadata": {
        "id": "GtTn6RJ2qZY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new df to work with chunks\n",
        "new_chapters_df = chapters_df\n",
        "\n",
        "#Get number of words in each chapter (helps to determine chunk length)\n",
        "ch_words = new_chapters_df[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append word counts to dataframe\n",
        "new_chapters_df[\"Word Count\"] = ch_words\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "iAbr54q_qZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize Text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "new_chapters_df['Tokens'] = new_chapters_df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "new_chapters_df"
      ],
      "metadata": {
        "id": "n1qjnGocqZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_ch = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = new_chapters_df['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Add to new list\n",
        "  chunked_ch.append(chunks)\n"
      ],
      "metadata": {
        "id": "SS6s8etzqZY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = new_chapters_df['Book + Chapter']\n",
        "values = chunked_ch\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ],
      "metadata": {
        "id": "rE0jzaDuqZY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_ch_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_ch_df.head()"
      ],
      "metadata": {
        "id": "ou_HFemhqZY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_ch_df = chunked_ch_df.stack().reset_index()\n",
        "chunked_ch_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "9hjJsF8tqZY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_ch_df['Book + Chunk'] = chunked_ch_df['Title'].astype(str) + ' Chunk ' + chunked_ch_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_ch_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "TreebankWordDetokenizer().detokenize\n",
        "\n",
        "chunked_ch_df['Text'] = chunked_ch_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_ch_df['Text'] \n",
        "\n",
        "#Lowercase all words\n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].str.lower()\n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_ch_df = chunked_ch_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "CUdh6TB3qZY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Disaggregating Texts\n",
        "\n",
        "**Why Disaggregate Texts?** \n",
        "\n",
        "Disaggregation is the breakdown of data into smaller (disordered) parts. In this case, disaggregation is accomplished through the alphabetization of the words in each chapter/chunk, and it creates data sets that can be used for further analysis (like topic modeling) and shared freely where original texts cannot be due to copyright restrictions. \n"
      ],
      "metadata": {
        "id": "vssDSEZLs_Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texts Sectioned by Chapter"
      ],
      "metadata": {
        "id": "bfxTcqvYjMmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHAPTER\n",
        "#Alphabetize words in each chapter string\n",
        "chapters_df['Text'] = chapters_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chapters_df"
      ],
      "metadata": {
        "id": "KnuyMCaTuIcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chapters to csv\n",
        "from google.colab import files\n",
        "\n",
        "chapters_df.to_csv('chapters_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapters_bow_output.csv')"
      ],
      "metadata": {
        "id": "fqWPBvnK44ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texts Sectioned by Chapter + Chunk"
      ],
      "metadata": {
        "id": "8LYQ1-NsjRat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_ch_df"
      ],
      "metadata": {
        "id": "1pwgXKNBjQnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_ch_df.to_csv('chapter_chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapter_chunks_bow_output.csv')"
      ],
      "metadata": {
        "id": "cjAHRphsjUqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texts Sectioned by Chunk"
      ],
      "metadata": {
        "id": "gx5K0DrijVm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Working with data from texts sectioned by CHUNK of N length\n",
        "#Alphabetize words in each chunk string\n",
        "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chunked_df"
      ],
      "metadata": {
        "id": "VfFCzNAO0kNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download disaggregated chunks to csv\n",
        "from google.colab import files\n",
        "\n",
        "chunked_df.to_csv('chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chunks_bow_output.csv')"
      ],
      "metadata": {
        "id": "7oZtM8aS2DBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Topic Modeling\n",
        "Below, Gensim is used to create LDA topic models of a pre-loaded dataframe of texts. Methods for evaluating topic coherence and analyzing topic output are also demonstrated. \n",
        "\n",
        "This code is adapted from [Intro to Topic Modeling with Gensim and pyLDAvis](https://github.com/hawc2/text-analysis-with-python/blob/master/Topic_Modeling.ipynb) and works well with input from from the Text Sectioning and Disaggregation code from [this repository](https://github.com/SF-Nexus/Extracted-Features/blob/main/Text_Sectioning_and_Disaggregation_in_Python.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ko6o7BcCtEJb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfm_YfxIRWjD"
      },
      "source": [
        "##Install Packages\n",
        "This code requires three main packages:\n",
        "- **NLTK:** Cleaning disaggregated data\n",
        "- **Gensim:** Preprocessing data and creating word embeddings, coherence models and topic models\n",
        "- **LDAvis:** Visualizing topic models\n",
        "\n",
        "Several other packages for wrangling and processing the data, such as io and pandas, will also be installed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNIhKdGeRWH1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZkNZc4lRhTL"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG2pqtVN_DFK"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjAj_KoORnfS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Corpus to New Data Frame and Clean\n",
        "\n",
        "This code will add the previously disaggregated corpus to a new data frame and begin to prepare it for topic modeling. "
      ],
      "metadata": {
        "id": "TmQacmhbrcMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy name of one of dataframes created above with chunked or chapterized data \n",
        "df = #chunked_df #chunked_ch_df #chapters_df"
      ],
      "metadata": {
        "id": "GXTU1j1zrxO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop any empty cells\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "_-KbPBb4sI3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add values in Text column to new list (for further cleaning)\n",
        "data = df.Text.values.tolist()"
      ],
      "metadata": {
        "id": "zm2SKPaQsJZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Texts\n",
        "Below are several preprocessing measures to help improve quality of text used for the topic model. Some may or may not be useful depending on specifications of corpus or goals of topic modeling:\n",
        "- **Stopword Removal:** Removes globally common words from corpus which may skew topics; some stopwords may be useful for analysis and some studies show that removing more than the top 10 most common words does not significantly impact topic model results \n",
        "- **Bigrams/Trigram Modeling:** Recognizes common 2- and 3-word phrases to prevent disambiguation (e.g. spaceship vs. space, ship); may misidentify phrases especially in corpora with large/uncommon vocabularies like sci-fi\n",
        "- **Lemmatization:** Transforms words to root form to aid recognition; some studies show may be redundant, unnecessary or even inaccurate in conflating differing words with same root meanings \n",
        "\n",
        "*Read More:*\n",
        "\n",
        "https://maria-antoniak.github.io/2022/07/27/topic-modeling-for-the-people.html\n",
        "\n",
        "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n"
      ],
      "metadata": {
        "id": "jSB23VIKpHgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define list of stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Add further stopwords by simply \"appending\" desired words to dictionary\n",
        "#stop_words.append('CHAPTER')"
      ],
      "metadata": {
        "id": "MQUc8j3cpG38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
      ],
      "metadata": {
        "id": "DpJErEdLpYln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function to perform simple preprocessing on text\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
      ],
      "metadata": {
        "id": "MyNO2S_7pbhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run processing function on texts\n",
        "data_words = list(sent_to_words(data))\n",
        "#print(data_words[:1])"
      ],
      "metadata": {
        "id": "5M668e3upd6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define stopword removal\n",
        "def remove_stopwords(texts):\n",
        "   return [[word for word in simple_preprocess(str(doc))\n",
        "if word not in stop_words] for doc in texts]\n",
        "\n",
        "#Define function to make bigrams\n",
        "def make_bigrams(texts):\n",
        "   return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "#def make_trigrams(texts):\n",
        "#   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "#Define function to lemmatize texts\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "   texts_out = []\n",
        "   for sent in texts:\n",
        "     doc = nlp(\" \".join(sent))\n",
        "     texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "   return texts_out"
      ],
      "metadata": {
        "id": "KFMsV5zvpef_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "ULprlcmDpgvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run functions to remove stopwords, make bigrams, and lemmatize text\n",
        "data_words = remove_stopwords(data_words)\n",
        "#data_words_bigrams = make_bigrams(data_nostops)\n",
        "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "#print(data_lemmatized[:4])"
      ],
      "metadata": {
        "id": "wOX5gnsNpi4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words"
      ],
      "metadata": {
        "id": "2Gt-HgRmpkdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Dictionary and Corpus\n",
        "Once the dataset is cleaned, two inputs must be created to run the topic model. Creating the dictionary maps every word in the corpus with a unique id number. The variable corpus is calculated by determining the frequency of each word in the document. Another optional cleaning measure can be used here--removing words that are extremely rare (existing in < n number of texts) or common (existing in > n number of texts).  "
      ],
      "metadata": {
        "id": "RKl-yFDEplFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dictionary\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(data_words)\n",
        "\n",
        "#Calculate term document frequency for each word in dataset\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data_words]\n",
        "corpus"
      ],
      "metadata": {
        "id": "hNjsLRstpnho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL cleaning: filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "#dictionary.filter_extremes(no_below=2, no_above=0.8)"
      ],
      "metadata": {
        "id": "H0NGM1AAptZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get number of unique tokens and documents\n",
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "metadata": {
        "id": "LgT-OGF6qGbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Optimal Number of Topics\n",
        "\n",
        "Calculating topic model coherence (i.e. similarity between the highest-scoring words in each topic) is one way to assess the optimal number of topics to use when creating and visualizing topic models. Below are two methods of calculating coherence: **C_V coherence**, which is calculated based on word co-occurences, and **U_Mass coherence**, which is calculated based on how frequently documents containing high-scoring words co-occur in the corpus. Both have been used in prior research and either may yield better results, depending on corpus specifications.\n",
        "\n",
        "Additional readings: \n",
        "\n",
        "https://aclanthology.org/D12-1087.pdf \n",
        "\n",
        "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
        "\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"
      ],
      "metadata": {
        "id": "SaICbjTHqKrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define list for model and coherence values\n",
        "coherence = []\n",
        "\n",
        "#Find coherence for set range of models and append to list\n",
        "for k in range(2,200):\n",
        "    print('Round: '+str(k))\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "    ldamodel = Lda(corpus, num_topics=k, \\\n",
        "               id2word = dictionary, eval_every = None)\n",
        "    \n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
        "         model=ldamodel, texts=data_words,\\\n",
        "         dictionary=dictionary, coherence='c_v')   \n",
        "                                                \n",
        "    coherence.append((k,cm.get_coherence()))"
      ],
      "metadata": {
        "id": "nz4XfXH3qLYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose coherence data\n",
        "x, y = np.array(coherence).T\n",
        "  \n",
        "      \n",
        "# plot our list in X,Y coordinates\n",
        "optimal, = plt.plot(x, y)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qmMoYFoGqRpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get coherence score for each num topics sorted from highest to lowest\n",
        "#Highest value will be optimal number of topics\n",
        "Data = {'Num Topics': optimal.get_xdata(), 'Coherence': optimal.get_ydata()}\n",
        "type(Data)\n",
        "df_optimal = pd.DataFrame.from_dict(Data)\n",
        "df_optimal.sort_values(by='Coherence',ascending=False).head()"
      ],
      "metadata": {
        "id": "YNgtOOLPqUnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U_Mass Coherence Calculation\n",
        "The function below calculates U_Mass coherence scores of n topic models run based on set parameters. Coherence is calculated on a range of -14 < x < 14 where lower-scoring models are more coherent. For example, a model with a score of -.4 is less coherent than a model with a -.9 score. \n"
      ],
      "metadata": {
        "id": "vD_QrVa9qXZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define list for model and coherence values\n",
        "coherence2 = []\n",
        "\n",
        "#Find coherence for set range of models and append to list\n",
        "for k in range(2,200):\n",
        "    print('Round: '+str(k))\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "    ldamodel = Lda(corpus, num_topics=k, \\\n",
        "               id2word = dictionary, eval_every = None)\n",
        "    \n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
        "         model=ldamodel, texts=data_words,\\\n",
        "         dictionary=dictionary, coherence='u_mass')   \n",
        "                                                \n",
        "    coherence2.append((k,cm.get_coherence()))"
      ],
      "metadata": {
        "id": "DD-_tzTSqVNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose coherence data\n",
        "x, y = np.array(coherence2).T\n",
        "        \n",
        "# plot our list in X,Y coordinates\n",
        "optimal2, = plt.plot(x, y)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-wsezLVzqagG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get coherence score for each num topics sorted from highest to lowest\n",
        "#Lowest value will be optimal number of topics\n",
        "Data = {'Num Topics': optimal2.get_xdata(), 'Coherence': optimal2.get_ydata()}\n",
        "type(Data)\n",
        "df_optimal2 = pd.DataFrame.from_dict(Data)\n",
        "df_optimal2.sort_values(by='Coherence',ascending=True).head()"
      ],
      "metadata": {
        "id": "_-DBZAZcqcZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Topic Models with Optimal Parameters\n",
        "Input the parameters of the topic model and run. The model has multiple parameters, including: \n",
        "- num_topics: Number of topics the model will generate (default = 100)\n",
        "- chunksize: Number of documents processed at a time (default = 2000)\n",
        "- passes: Number of times model is trained on corpus (default = 1)\n",
        "- iterations: Number of times model \"loops\" over each document (default = 50)\n",
        "\n",
        "Calculating coherence (above) helps determine topic number, and other methods can be used to determine appropriate values for the other parameters. \n",
        "\n",
        "**Chunk size:** Setting chunk size to a larger number than that of documents in the model ensures that all documents are processed at once (though this requires enough memory space). \n",
        "\n",
        "**Passes and Iterations:** A common way to determine the best number of passes and iterations is by training a topic model and checking the \"log\" to see the document convergence rate (what percentage of topic/word assignments attain stability). If convergence is low, increase number of passes and interations. \n",
        "\n",
        "In general, as chunksize increases, passes and iterations should increase as well. Also keep in mind that corpus size may effect number of topics--in the cases of smaller corpora, using too many topics will likely make them too general OR too limited to the context of only one text. Consider running multiple models and comparing coherence, perplexity, and top words per topic. \n",
        "\n",
        "Additional reading: \n",
        "\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel"
      ],
      "metadata": {
        "id": "8yPbLlJ8qfe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import logging to gauge passes and iterations; will output file to working directory\n",
        "import logging\n",
        "logging.basicConfig(filename='gensim.log',\n",
        "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
        "                    level=logging.INFO)"
      ],
      "metadata": {
        "id": "vwbCuH_RqeSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 99\n",
        "chunksize = 10000\n",
        "passes = 200\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=1\n",
        ")"
      ],
      "metadata": {
        "id": "kLUw7ehyqi6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute C_V Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "TCYxkKvFqnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examine Topic Model Output\n",
        "Once the model has been run, it is possible to retrieve the top words in each topic and visualing the model. These are methods can help further assess model coherence and point to future directions for analysis:\n",
        "- **Top Words Per Topic:** Evaluate to what extent each topic contains semantically similar words, how/why these words might be meaningful in context of corpus\n",
        "- **Visualizations:** Determine topic relatedness (how far apart topic circles are on plane) and topic prevalence (how large circles are corresponds to topic prevaence in corpus)\n",
        "\n",
        "Additional reading: \n",
        "\n",
        "https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#15visualizethetopicskeywords\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n"
      ],
      "metadata": {
        "id": "5qrKa1r9qqDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print n number of words in each topic\n",
        "for idx, topic in lda_model.print_topics(num_words=8):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "metadata": {
        "id": "OXD1btIcqpRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIONAL: Define word cloud function\n",
        "def create_wordcloud(model, topic):\n",
        "    text = {word: value for word, value in model.show_topic(topic)}\n",
        "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
        "    wc.generate_from_frequencies(text)\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Topic\" + \" \"+ str(topic))\n",
        "    plt.show()\n",
        "    \n",
        "#Ignore depreciation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Create word clouds\n",
        "for i in range(1,num_topics):\n",
        "    create_wordcloud(lda_model, topic=i)"
      ],
      "metadata": {
        "id": "tgE_thMkqsPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create visualization of topic model above \n",
        "%matplotlib inline\n",
        "import pyLDAvis.gensim_models\n",
        "vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary)\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "i6aQBQLhqx6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save visualization as html output\n",
        "pyLDAvis.save_html(vis, '/content/LDAviz.html')"
      ],
      "metadata": {
        "id": "iluH9y6vq3nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Top Topics Per Document\n",
        "\n",
        "Find the topic with highest percentage in each document in corpus. \n",
        "\n",
        "Additional reading: \n",
        "https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf"
      ],
      "metadata": {
        "id": "gHTWRPDgq8cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function that retrieves dominant topic for each document and puts in dataframe\n",
        "def format_topics_texts(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    text_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                text_topics_df = text_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    text_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    text_topics_df = pd.concat([text_topics_df, contents], axis=1)\n",
        "    return(text_topics_df)"
      ],
      "metadata": {
        "id": "dTI7k5d5q7t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run dominant topic function on corpus\n",
        "df_topic_texts_keywords = format_topics_texts(ldamodel=lda_model, corpus=corpus, texts=data_words)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_texts_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ],
      "metadata": {
        "id": "hAg9AV5cq_YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Top Documents Per Topic\n",
        "Calculate the top documents attributed to each topic in the model. \n",
        "\n",
        "Additional reading:\n",
        "\n",
        "https://stackoverflow.com/questions/63777101/topic-wise-document-distribution-in-gensim-lda\n",
        "\n",
        "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_methods.ipynb \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#7.-The-most-representative-sentence-for-each-topic "
      ],
      "metadata": {
        "id": "Utjpsi0irCED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add Book + Chapter labels to dataframe for easier ID\n",
        "doc_names = df['Book + Chapter']\n",
        "df_topic_texts_keywords = df_topic_texts_keywords.join(doc_names)"
      ],
      "metadata": {
        "id": "Mc2d9WPlrGdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get most representative text for each topic \n",
        "#Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "#Create new dataframe and group topic keywords by dominant topic column\n",
        "topics_sorted_df = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_texts_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "#Sort data by percent contribution and select highest n values for each topic\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    topics_sorted_df = pd.concat([topics_sorted_df, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                            axis=0)"
      ],
      "metadata": {
        "id": "IuD9T797rJAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset Index of new df\n",
        "topics_sorted_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\", \"Text Name\"]\n",
        "topics_sorted_df = topics_sorted_df.reindex(columns=['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text Name\", \"Representative Text\"])\n",
        "# Show\n",
        "topics_sorted_df.head()"
      ],
      "metadata": {
        "id": "aWK-vZXdrK95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future step: Gensim word embeddings!\n",
        "\n",
        "https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92 \n",
        "\n",
        "https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/"
      ],
      "metadata": {
        "id": "INKONFXkrM_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources\n",
        "\n",
        "**More Examples of Topic Modeling Research:**\n",
        "\n",
        "*   http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html\n",
        "*   http://www.cs.columbia.edu/~blei/papers/Blei2011.pdf\n",
        "*   https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf \n",
        "\n",
        "**More Topic Modeling Tools:**\n",
        "\n",
        "*   https://github.com/polsci/colab-gensim-mallet/blob/master/topic-modeling-with-colab-gensim-mallet.ipynb\n",
        "*   https://github.com/laurejt/authorless-tms \n",
        "*   https://colab.research.google.com/github/kldarek/skok/blob/master/_notebooks/2021-05-27-Topic-Models-Introduction.ipynb\n"
      ],
      "metadata": {
        "id": "0KYWtDBrrQrl"
      }
    }
  ]
}